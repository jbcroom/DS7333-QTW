---
title: 'MSDS 7333 Fall 2020: Case Study 1'
author: "Jayson Barker, Brandon Croom, Shane Weinstock"
output:
  pdf_document: default
  html_document: default
---

# Business Understanding

Organizations measure productivity in various ways depending upon the metrics they want to track. They may look at shipping times for materials, optimization of workflows within the organization, and any are focused on cost minimization. One such way organizations can help within this optimization is through leveraging real-time location systems (RTLS). Real-time location systems allow organizations to monitory assess and workers through the workflows that exist within the organization. 

The evaluation of RTLS as is relates to an organizations WiFi usage is one such method to analyze productivity. More specifically, the distribution of various wifi-enabled RTLS devices across a facility may allow clustering methods to be leveraged in order to predict the position of assets based on previous patterns. In this particular evaluation data were provided for offline and online data across 6 access points. The building layout, access point configuration, online measurements (black dots), and offline measurements (gray dots) are shown below in Figure 1.

![Figure 1: Building Floorplan](building.png)

The objectives for this case study are:

* evaluate the data to determine the optimal access point configuration to include in the data
* evaluate both a regular unweighted KNN and weighted K-Nearest Neighbor (KNN) methodology to perform predictions


# Data Engineering

In evaluating the data that has been made available it is critical to understand the data file structure. There are two data files to work with: Online.txt and Offline.txt. The Offline.txt file contains information relating to the offline measurements that were taken. The Online.txt file contains information relating to the online measurements that were taken. The Offline.txt file makes up the training dataset while the Online.txt file is the test dataset. Both files are structured with the data fields noted below:

Data Field | Data Description
-----------|-----------------
   t       | Time stamp in milliseconds
   Id      | Router MAC Address
   Pos     | Router Location (Comma separated positions for x, y, z)
   Degree  | Direction scanning device measured in degrees that was carried by the researcher
   MAC     | MAC address of either the router, or scanning device combined with corresponding values for signal strength (dBm), the mode in which it was operating(adhoc scanner = 1, access router = 3), and its corresponding channel frequency.
   Signal  | Received Signal Strength in DbM
   
   
As noted above, the data fields have compressed information into single columns. For example, the Pos column contains information for X,Y, and Z coordinates in comma delimited format instead of being broken out into individual fields. The same is true for the MAC column. The MAC column contains information on the router MAC address, signal strength, operating mode and channel. In order to handle all of this information dataframes were constructed for both the online and offline datasets. Each field was split out into a column within the dataframe in order to make analysis much easier. 

In reviewing the data in raw form a few observations were identified:

 * The Z position (i.e. elevation) was zero for the data points. This indicates a column that can be dropped for the specific analysis.
 * The scan angles are not consistent and need to be made consistent. Plan to round these to the nearest 45 degree angle,
 * There are some MAC addresses with only a few data points. A threshold of approximately 1 million will be set. Any MAC addresses with less than this threshold will be removed
 
In specific relation to this analysis the following features will also be dropped from the dataframe as they do not impact the analysis:

 * scanMac - this is the MAC address of the scanner
 * channel - this is the channel where the reading was acquired
 * type - this is the device type
 
Multiple columns will be created to assist in further analysis
 
 * medSignal - this will be the median signal strength captured
 * avgSignal - this will be the average signal strength captured
 * sdSignal - this will be the standard deviation of the captured signal strength
 * iqrSignal - this is the interquartile range of the captured signal strength
 * posXY - this is a combination of the posX, posY fields into a single value. Coordinates are separated by the '-' character
 * rawTime - this is a raw time value

In addition to the above the angle column was standardized to the nearest 45 degree angle. Due to multiple differences in the angle readings, this standardization is necessary to ensure consistency

For purposes of these analysis these rules were applied to both the online and offline datasets. 

```{r,echo=FALSE}
# NOTE: Code portions leveraged from the course provided code in certain instances. 
# Define the MAC IDs of the relevant routers/access points
fileLoc = "C:\\RAI\\DS7333-QTW\\CaseStudy1\\Data\\offline.final.trace.txt"
fileLoc2 = "C:\\RAI\\DS7333-QTW\\CaseStudy1\\Data\\online.final.trace.txt"
macsC0 = c('00:0f:a3:39:e1:c0', '00:14:bf:b1:97:90', '00:14:bf:3b:c7:c6','00:14:bf:b1:97:81', '00:14:bf:b1:97:8a', '00:14:bf:b1:97:8d') #C0
macsCD = c('00:0f:a3:39:dd:cd', '00:14:bf:b1:97:90', '00:14:bf:3b:c7:c6','00:14:bf:b1:97:81', '00:14:bf:b1:97:8a', '00:14:bf:b1:97:8d') #CD
macsC0_CD = c('00:0f:a3:39:e1:c0', '00:0f:a3:39:dd:cd', '00:14:bf:b1:97:90','00:14:bf:3b:c7:c6', '00:14:bf:b1:97:8a', '00:14:bf:b1:97:8d') # C0 & CD
cols01 = c("posXY", "posX","posY", "orientation", "angle") 
cols02 = c("posXY", "posX","posY")
cols03 = "signal"
cols04 = c("posX", "posY")
cols05 = "avgSignal"
cols06 = c("time", "scanMac","posX", "posY", "posZ", "orientation","mac", "signal", "channel", "type")
cols07 = c("scanMac", "posZ", "channel", "type")
cols08 = c("time", "posX", "posY", "orientation", "signal")
cols09 = c("POSIXt", "POSIXct")
cols10 = c("posXY", "posX","posY", "orientation", "angle")
commentValue = "[;=,]"
commentValue2 = "#"
sepValue = "-"
accessPoint = "3"
seed = 125

# This takes all of the above and makes a function that combines all of it
processLine = function(x)
{ tokens = strsplit(x, commentValue)[[1]]
  if (length(tokens) == 10) return(NULL)
  tmp = matrix(tokens[ - (1:10) ], ncol= 4, byrow = TRUE)
  cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow(tmp), 6,
               byrow = TRUE), tmp)}

roundOrientation = function(angles) {
  refs = seq(0, by = 45, length  = 9)
  q = sapply(angles, function(o) which.min(abs(o - refs)))
  c(refs[1:8], 0)[q]}

readData =
  function(filename, macs )
  {txt = readLines(filename)
    lines = txt[ substr(txt, 1, 1) != commentValue2 ]
    tmp = lapply(lines, processLine)
    offline = as.data.frame(do.call("rbind", tmp),
                            stringsAsFactors= FALSE)
    names(offline) = cols06
    offline = offline[ offline$type == accessPoint, ]
    dropVars = cols07
    offline = offline[ , !( names(offline) %in% dropVars ) ]
    offline = offline[ offline$mac %in% macs, ]
    numVars = cols08
    offline[ numVars ] = lapply(offline[ numVars ], as.numeric)
    offline$rawTime = offline$time
    offline$time = offline$time/1000
    class(offline$time) = cols09
    # round orientations to nearest 45
    offline$angle = roundOrientation(offline$orientation)
    return(offline)}

# With C0 without CD
offline <- readData(filename = fileLoc , macs=macsC0)   # Change file location
offline$posXY = paste(offline$posX, offline$posY, sep = sepValue)

byLocAngleAP = with(offline, by(offline, list(posXY, angle, mac),function(x) x))

signalSummary = 
  lapply(byLocAngleAP, 
         function(oneLoc) { 
           ans = oneLoc[1,] 
           ans$medSignal = median(oneLoc$signal) 
           ans$avgSignal = mean(oneLoc$signal) 
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
         })
offlineSummary = do.call("rbind", signalSummary)

# With CD without C0
offline2 <- readData(filename = fileLoc , macs=macsCD)  # Change file location
offline2$posXY = paste(offline2$posX, offline2$posY, sep = sepValue)

byLocAngleAP2 = with(offline2, by(offline2, list(posXY, angle, mac), function(x) x))

signalSummary2 = 
  lapply(byLocAngleAP2, 
         function(oneLoc) { 
           ans = oneLoc[1,] 
           ans$medSignal = median(oneLoc$signal) 
           ans$avgSignal = mean(oneLoc$signal) 
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
         })
offlineSummary2 = do.call("rbind", signalSummary2)

# With C0 and CD
offlineall <- readData(filename = fileLoc , macs=macsC0_CD)   # Change file location
offlineall$posXY = paste(offlineall$posX, offlineall$posY, sep = sepValue)

byLocAngleAPall = with(offlineall, by(offlineall, list(posXY, angle, mac),function(x) x))

signalSummaryall = 
  lapply(byLocAngleAPall, 
         function(oneLoc) { 
           ans = oneLoc[1,] 
           ans$medSignal = median(oneLoc$signal) 
           ans$avgSignal = mean(oneLoc$signal) 
           ans$num = length(oneLoc$signal)
           ans$sdSignal = sd(oneLoc$signal)
           ans$iqrSignal = IQR(oneLoc$signal)
           ans
         })
offlineSummaryall = do.call("rbind", signalSummaryall)

# With C0 without CD
online <- readData(filename = fileLoc2, macs=macsC0)  
online$posXY = paste(online$posX, online$posY, sep = sepValue)

Loc = with(online, by(online, list(posXY), function(x) { 
  ans = x[1, cols01] 
  avgSS = tapply(x$signal, x$mac, mean) 
  y = matrix(avgSS, nrow = 1, ncol = 6, dimnames = list(ans$posXY, names(avgSS))) 
  cbind(ans, y) }))
onlineSummary = do.call("rbind", Loc)

# With CD without C0
online2 <- readData(filename = fileLoc2, macs=macsCD)  
online2$posXY = paste(online2$posX, online2$posY, sep = sepValue)

Loc = with(online2, by(online2, list(posXY), function(x) {
  ans = x[1, cols01] 
  avgSS = tapply(x$signal, x$mac, mean) 
  y = matrix(avgSS, nrow = 1, ncol = 6, 
             dimnames = list(ans$posXY, names(avgSS))) 
  cbind(ans, y) }))
onlineSummary2 = do.call("rbind", Loc)

# With both CD and C0
onlineall <- readData(filename = fileLoc2, macs=macsC0_CD)  
onlineall$posXY = paste(onlineall$posX, onlineall$posY, sep = sepValue)

Loc = with(onlineall, by(onlineall, list(posXY), function(x) {
  ans = x[1, cols01] 
  avgSS = tapply(x$signal, x$mac, mean) 
  y = matrix(avgSS, nrow = 1, ncol = 6, 
             dimnames = list(ans$posXY, names(avgSS))) 
  cbind(ans, y) }))
onlineSummaryAll = do.call("rbind", Loc)
```

As part of the analysis the determination of certain access points to keep or remove was needed. Specifically evaluation of whether to individually keep both access points that ended in C0 and CD or to keep only one of these points. This need resulted in three distinct dataframes for each file as follows:

Dataframe | Description
-----------|------------
offlineSummary    | The offline dataframe containing only the C0 access point
offlineSummary2   | The offline dataframe containing only the CD access point
offlineSummaryall | The offline dataframe containing both C0 and CD
onlineSummary     | The online dataframe containing only the C0 access point
onlineSummary2    | The online dataframe containing only the CD access point
offlineSummaryall | The online dataframe containing both C0 and CD

All of the dataframes have the same structure. For illustrative purposes a view of the offlineSummaryall dataframe showing the available features is below:

```{r}
summary.default(offlineSummaryall)
```

In preparation for data modeling, the data has also been flattened to create a dataframe with a single positional entry (X, Y coordinate) for each MAC address. This will help to reduce the analysis record count down to approximately 1328 points per MAC address. this breakdown is shown below:


```{r, echo=FALSE}
head(offlineSummaryall)

aggregate(cbind(count = mac) ~ mac, 
          data = offlineSummaryall, 
          FUN = function(x){NROW(x)})
```
Building the dataframes as described above with flattened data and dataframes containing the different access points allows for providing quick analysis of the data. 

# Access Point Analysis

One of the analysis points that must be determined is whether to use the MAC address ending with C0 only, the MAC address ending in CD only, or to keep both in our dataset. In order to determine which approach to take, KNN methodology will be executed on all three configurations. Using this information it can be determined which data configuration will be used based on the error rates output by executing KNN. For each data configuration k will be selected from 1 through 20. This is an arbitrary selection but should provide enough data points to make an analysis. Initially the elbow plots for the three scenarios will be graphed together to assist in determining which scenario best fits.
```{r,echo=FALSE}
## K-Nearest Neighbors Analysis

### Angle orientation adjustments before KNN
adjustData = function(data, varSignal = cols03, cols = cols02) {
  set01 = with(data, by(data, list(posXY), function(x) {
  ans = x[1, cols]
  avgSS = tapply(x[ , varSignal ], x$mac, mean)
  y = matrix(avgSS, nrow = 1, ncol = 6,
  dimnames = list(ans$posXY,names(avgSS)))
  cbind(ans, y)}))
  adjustDataRes = do.call("rbind", set01)
return(adjustDataRes)}

### Angle orientation adjustments before KNN - odd and evens
adjustAngle = function(adjustAngleData, signals = NULL, m = 1){
  refs = seq(0, by = 45, length  = 8)
  roundedAngle = roundOrientation(adjustAngleData)
  
  if (m %% 2 == 1) angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
  else {m = m + 1
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
    if (sign(adjustAngleData - roundedAngle) > -1) 
      angles = angles[ -1 ]
    else 
      angles = angles[ -m ]}
  angles = angles + roundedAngle
  angles[angles < 0] = angles[ angles < 0 ] + 360
  angles[angles > 360] = angles[ angles > 360 ] - 360
  angles = sort(angles)
  offlineSubset = signals[ signals$angle %in% angles, ]
  adjustData(offlineSubset, varSignal = cols05)}

### Neighbor location
locKNNpoints = function(signalData, trainSubset) { 
  diffs = apply(trainSubset[, 4:9], 1, function(x) x - signalData) 
  dists = apply(diffs, 2, function(x) sqrt(sum(x^2))) 
  closest = order(dists) 
  return(trainSubset[closest, 1:3])}

### Location prediction
knnPredict = function(signalData, newAngles, trainData,numAngles = 1, k = 3){closeXY = list(length = nrow(signalData))
  for (i in 1:nrow(signalData)) {
    trainSS = adjustAngle(newAngles[i], trainData, m = numAngles)
    closeXY[[i]] = locKNNpoints(signalData = as.numeric(signalData[i, ]), trainSS)}
    estXY = lapply(closeXY,function(x) sapply(x[ ,2:3],function(x) mean(x[1:k])))
    estXY = do.call("rbind", estXY)
  return(estXY)}
```

```{r,echo=FALSE}
### C0 KNN Scoring - Updated original code to loop for k=1-15 and maintain errors for elbow plotting
onlineLocs_C0 = onlineSummary[ , cols04]
wss_C0 <- (nrow(onlineLocs_C0)-1)*sum(apply(onlineLocs_C0,2,var))


kMax = 20

for (i in 1:kMax){
  predLocs = knnPredict(signalData = onlineSummary[ , 6:11], newAngles = onlineSummary[ , 4], offlineSummary, numAngles = 3, k = i)
  C0error = function(predLocs, onlineLocs_C0) sum(rowSums((predLocs - onlineLocs_C0)^2)) 
  wss_C0[i] = sapply(list(predLocs), C0error, onlineLocs_C0)
}

### CD KNN Scoring - BC
onlineLocs_CD = onlineSummary2[ , cols04]
wss_CD <- (nrow(onlineLocs_CD)-1)*sum(apply(onlineLocs_CD,2,var))


for (i in 1:kMax){
  predLocs = knnPredict(signalData = onlineSummary2[ , 6:11], newAngles = onlineSummary2[ , 4], offlineSummary2, numAngles = 3, k = i)
  CDerror = function(predLocs, onlineLocs_CD) sum(rowSums((predLocs - onlineLocs_CD)^2)) 
  wss_CD[i] = sapply(list(predLocs), CDerror, onlineLocs_CD)
}

### C0 KNN Scoring - BC
onlineLocs_C0CD = onlineSummaryAll[ , cols04]
wss_C0CD <- (nrow(onlineLocs_C0CD)-1)*sum(apply(onlineLocs_C0CD,2,var))


for (i in 1:kMax){
  predLocs = knnPredict(signalData = onlineSummaryAll[ , 6:11], newAngles = onlineSummaryAll[ , 4], offlineSummaryall, numAngles = 3, k = i)
  C0CDerror = function(predLocs, onlineLocs_C0CD) sum(rowSums((predLocs - onlineLocs_C0CD)^2)) 
  wss_C0CD[i] = sapply(list(predLocs), C0CDerror, onlineLocs_C0CD)
}
```

```{r, echo=FALSE}
# Plot the elbow curves for all three KNN runs to determine the best K
plot(1:kMax, wss_C0, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares",main="Elbow Curve for KNN", col="blue",pch="o")
points(1:kMax, wss_CD, col="red", pch="*")
lines(1:kMax, wss_CD, col="red",lty=2)
points(1:kMax, wss_C0CD, col="red", pch="*")
lines(1:kMax, wss_C0CD, col="red",lty=2)
legend("topright",legend=c("C0","CD","COCD"), col=c("blue","red","black"),
                                   pch=c("o","*","+"),lty=c(1,2,3), ncol=1)

```

Viewing the combined elbow plot above it can be seen that for the CD configuration a k=4 looks to be about the optimal solution. For C0, k=8 looks to be optimal. From the combined graph, it's difficult to tell what the optimal K for COCD should be. Evaluating the plots individually will assist in evaluating the scenarios.

```{r, echo=FALSE}
plot(1:kMax, wss_C0, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares",main="Elbow Curve for C0 Only KNN", col="blue",pch="o")

plot(1:kMax, wss_CD, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares",main="Elbow Curve for CD Only KNN", col="blue",pch="o")

plot(1:kMax, wss_C0CD, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares",main="Elbow Curve for COCD KNN", col="blue",pch="o")
```
Evaluating the individual graphs reaffirms the values for C0 and CD as standalone cases; k=8 and k=4 respectively. The combined case gives a k=7 that looks to be optimal. Lets now look at the individual errors at these k values to determine which best meets the analysis needs.


```{r, echo=FALSE}
sprintf("CO KNN Error at K=5: %f", wss_C0[8])
sprintf("CD KNN Error at K=4: %f", wss_CD[4])
sprintf("COCD KNN Error at K=7: %f", wss_C0CD[7])
```
From the output above it can be seen that the combination with CO and CD has the highest error. This would lead to not keeping both values in the mix. Given CD by itself has the lowest error this seems the most likely option to address. 

# Weighted KNN

Now that it's been assessed that the dataset with just CD looks to be the best another verification will be performed on this dataset using a weighted KNN. Leveraging a weighted KNN should allow for improving the accuracy of predicting locations. In order to perform the weighting, the signal strength column will be used as the weighting factor. Using this methodology allows for stronger, closer signals to have a larger impact on determining location than those with weaker signals. This is different than the simple KNN used above in that locations of closer distance may now have a larger impact to the neighbor grouping that those that are further away.

In order to make the weighted KNN work as expected our current data structures can be leveraged. The structure of the current dataframes is adequate to support the weighted KNN methodology. Some engineering is required within individual functions as part of the analysis; largely these engineering changes are modifications to compute the weight and output distance values based on the weighting calculations.

As part of this weighted KNN analysis cross fold validation will also be leveraged. In this particular run 11 fold validation will be leveraged. As with the previous simple KNN calculations k values from 1 to 20 will be leveraged in order to determine the optimal number of neighbors for this analysis. 

```{r, echo=FALSE}
#### KNN Distance Weighting

### To further improve the accuracy of predicting location, we up-weight stronger signals relative to weaker signals. 
### This allows us to give strong, closer signals a larger impact on determining the location than those that are weak and further away. 
### We use the weighting fraction below to achieve this result, where we use the signal strength as an estimator for distance.

### The locKNNpoints() function below has been modified from the previous version to also output the distance metrics utilized in 
### determining the "closest" k points. Additionally, the knnPredict() function has been modified to compute weights with the formula 
### above and distance outputs from the locKNNpoints() output. These weights are multiplied against each k nearest observation, 
### respectively, then summed to compute a weighted estimation by distance. These new calculations may allow locations of closer 
### distances to be more impactful than those further away, instead of equal weighting amongst the k points.
#Modify findNN to also output the distances


v = 11
permuteLocs = sample(unique(offlineSummary$posXY))
permuteLocs = matrix(permuteLocs, ncol = v, 
                     nrow = floor(length(permuteLocs)/v))

onlineFold = subset(offlineSummary, posXY %in% permuteLocs[ , 1])
onlineFold.ul <- nrow(unique(onlineFold[,2:3]))


### Previously we structured and summarized the offline data into six signal strength columns, one for each access point.  
### We will do the same with our cross-validated test data. However, because it is easier to structure the test data in its complete 
### form from offline data which is then divided into our desired folds, we now need to modify the reshapeSS() function as follows.

seed = 101
reshapeSS = function(data, varSignal = "signal", 
                     keepVars = c("posXY", "posX","posY"),
                     sampleAngle = FALSE, 
                     refs = seq(0, 315, by = 45)) {
  
  set.seed(seed = seed)
  
  byLocation =
    with(data, by(data, list(posXY), 
                  function(x) {
                    if (sampleAngle) { # conditional statement added for cross-validation
                      x = x[x$angle == sample(refs, size = 1), ]}
                    ans = x[1, keepVars]
                    avgSS = tapply(x[ , varSignal ], x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY,
                                               names(avgSS)))
                    cbind(ans, y)
                  }))
  newDataSS = do.call("rbind", byLocation)
  return(newDataSS)
  
  keepVars = c("posXY", "posX","posY", "orientation", "angle")
}


KeepVars = c("posXY", "posX","posY", "orientation", "angle")

onlineCVSummary = reshapeSS(offline, keepVars = keepVars,sampleAngle = TRUE)
onlineCVSummary2 = reshapeSS(offline2, keepVars = keepVars,sampleAngle = TRUE)


findNN = function(newSignal, trainSubset) {
  diffs = apply(trainSubset[ , 4:9], 1, 
                function(x) x - newSignal)
  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) )
  closest = order(dists)
  return(list(trainSubset[closest, 1:3 ],dists[order(dists)]))
}

#Modify findNN to utilize distance output for estimation efforts
predXY = function(newSignals, newAngles, trainData, 
                  numAngles = 1, k = 3){
  
  closeXY = list(length = nrow(newSignals))
  closeDist = list(length = nrow(newSignals))
  
  for (i in 1:nrow(newSignals)) {
    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)
    fnnResult =
      findNN(newSignal = as.numeric(newSignals[i, ]), trainSS)
    
    closeXY[[i]] = fnnResult[[1]]
    closeDist[[i]] = fnnResult[[2]]
  }
  
  
  distWeight = list(length = length(closeDist))
  
  for (i in 1:length(closeDist)){
    distW = list(length = k)
    for (j in 1:k){
      distW[j] = (1/closeDist[[i]][j])/sum(1/closeDist[[i]][1:k])
    }
    
    distWeight[[i]] =  distW
  }
  estXYDetails = list(length=length(closeXY))
  
  for(i in 1:length(closeXY)){
    estXYDetails[[i]] = as.matrix(closeXY[[i]][1:k,2:3]) * unlist(distWeight[[i]])
  }
  
  estXY = lapply(estXYDetails,
                 function(x) apply(x, 2,
                                   function(x) sum(x)))
  
  estXY = do.call("rbind", estXY)
  return(estXY)
}


set.seed(seed = seed)
# set K here to kMax defined earlier to keep #K's in sync
K = kMax
err = rep(0, K)
for (j in 1:v) {
  onlineFold = subset(onlineCVSummary2, 
                      posXY %in% permuteLocs[ , j])
  offlineFold = subset(offlineSummary2,
                       posXY %in% permuteLocs[ , -j])
  actualFold = onlineFold[ , c("posX", "posY")]
  for (k in 1:K) {
    estFold = predXY(newSignals = onlineFold[ , 6:11],
                     newAngles = onlineFold[ , 4], 
                     offlineFold, numAngles = 3, k = k)
    err[k] = err[k] + calcError(estFold, actualFold)
  }
}


###  will perform similar steps and perform v-fold tests on one to twenty k nearest neighbor estimations to find the optimal value of K with 
### weighted samples.

set.seed(seed = seed)
oldPar = par(mar = c(4, 5, 1, 1))
plot(y = err, x = (1:K),  type = "l", lwd= 2,
     ylim = c(1000, 2100),
     xlab = "Number of Neighbors",
     ylab = "Sum of Square Errors")
rmseMin = min(err)
kMin2 = which(err == rmseMin)[1]
segments(x0 = 0, x1 = kMin2, y0 = rmseMin, col = gray(0.4), 
         lty = 2, lwd = 2)
segments(x0 = kMin2, x1 = kMin2, y0 = 900,  y1 = rmseMin, 
         col = grey(0.4), lty = 2, lwd = 2)
mtext(kMin2, side = 1, line = 1, at = kMin2, col = grey(0.4))
text(x = kMin2 - 2, y = rmseMin + 40, 
     label = as.character(round(rmseMin)), col = grey(0.4))


### Plotting error values for 20 distinct k test iterations, we see that k=8 produces optimal results.

actualXY_2 = onlineSummary2[ , c("posX", "posY")]
AP = matrix( c( 7.5, 6.3, 2.5, -.8, 12.8, -2.8,  
               1, 14, 33.5, 9.3,  33.5, 2.8),
            ncol = 2, byrow = TRUE,
            dimnames = list(macsCD, c("x", "y") ))

trainPoints = offlineSummary[ offlineSummary$angle == 0 & 
                              offlineSummary$mac == "00:0f:a3:39:dd:cd" ,
                        c("posX", "posY")]
oldPar = par(mar = c(1, 1, 1, 1))

set.seed(seed = seed)
estXYk8 = predXY(newSignals = onlineSummary2[ , 6:11], 
                 newAngles = onlineSummary2[ , 4], 
                 offlineSummary2, numAngles = 3, k =kMin2)
SSE.k8 <- calcError(estXYk8, actualXY_2)
```
The elbow plot above shows that k=8 seems to be the optimal number of neighbors for this weighted KNN evaluation. Furthermore, evaluating the error present at k=8 will allow for comparison between the simple KNN and weighted KNN.
```{r}
sprintf("CD Weighted KNN Error at K=8: %f", SSE.k8)
```
Comparing the weighted KNN error (shown above) for KD and the simple KNN error (shown below) for CD we can see the errors are roughly similar. 

```{r}
sprintf("CD KNN Error at K=4: %f", wss_CD[4])
```
The distance calculation for the weighted KNN can also be visualized, as shown below. This visualization allows for determining how the proximity of points is pulled together.

```{r}
floorErrorMap = function(estXY, actualXY, trainPoints = NULL, AP = NULL){
  
    plot(0, 0, xlim = c(0, 35), ylim = c(-3, 15), type = "n",
         xlab = "", ylab = "", axes = FALSE, main="Weighted KNN Visualization")
    box()
    if ( !is.null(AP) ) points(AP, pch = 15)
    if ( !is.null(trainPoints) )
      points(trainPoints, pch = 19, col="grey", cex = 0.6)
    
    points(x = actualXY[, 1], y = actualXY[, 2], 
           pch = 19, cex = 0.8 )
    points(x = estXY[, 1], y = estXY[, 2], 
           pch = 8, cex = 0.8 )
    segments(x0 = estXY[, 1], y0 = estXY[, 2],
             x1 = actualXY[, 1], y1 = actualXY[ , 2],
             lwd = 2, col = "red")
}
floorErrorMap(estXYk8, onlineSummary[ , c("posX","posY")], 
              trainPoints = trainPoints, AP = AP)
```
# Conclusion
In conclusion the analysis was able to leverage a simple KNN to evaluate whether to maintain the C0, CD or both MAC addresses. Through the analysis it was determined that CD was the MAC address that should be maintained due to having the least calculated errors. The dataset containing CD was then passed through a weighted KNN model to determine if leveraging the signal strength for location had any impact on the evaluation. The weighted KNN showed that eight neighbors was the optimal number of neighbors for the analysis. 

Using the KNN methodology (simple or weighted) does have some drawbacks when applied to RTLS. Speed in one predominant factor that comes into play. KNN is not speedy and in this particular instance it was required to aggregate the data on angle of orientation to obtain a speed boost for processing. The speed boost was necessary to make this analysis work however for true real time analysis this may not be enough for real time tracking. 

The angles provided in the data were not overly useful for the KNN method. Given that the devices were always held at multiple angles based on user preference. Having this information did not add any value as the main goal was to determine location. The angle may be more applicable if the analysis was over multiple floors and elevation came into play. 

In order to overcome some of these perceived drawbacks a few options could be implemented. Caching of the training data at the angle sweeps to prevent recreation of data would dramatically help improve processing speed. Increasing the size of the testing test by including the cached angle information would allow for the creation of many kNN models and then select the one that best met the needs. This would help improve both speed an accuracy. 

Varying the number of angles included in the training aggregation would also be an interesting analysis to perform. In the current analysis angles were managed in 45 degree increments. An argument could be made that including all available angles would be ideal to prevent "data loss", however there could be a performance impact leveraging this approach. 
   