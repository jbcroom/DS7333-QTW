---
title: 'MSDS 7333 Fall 2020: Case Study 3'
author: "Jayson Barker, Brandon Croom, Shane Winestock"
output:
  html_document: default
  pdf_document: default
---

# Business Understanding
Spam email is unsolicited and unwanted junk email sent out in bulk to an indiscriminate recipient list. Spam is typically sent out for commercial purposes, but may also be leveraged for nefarious purposes such as phishing attempts and scams. These emails are typically sent out in bulk by botnets which are networks of infected computers. According to multiple internet sites, spam email accounts for 14.5 billion email message globally per day. This is approximately 45% of all emails. This magnitude of spam emails mixes in with regular emails that users need to address for business or personal purposes, thus making email more difficult and even risky for unaware users to use.

To overcome the magnitude of spam emails and the interference with regular emails, being able to classify an email as spam or not-spam (aka ham) becomes a necessity. This problem has been solved in multiple ways by various technology vendors such as Microsoft and Google. For the purposes of this report, the project team will setup to classify emails as spam or not-spam through decision tree based approaches, using the data which includes indicators denoting spam messages. 

A tree provides a visual representation of a course of action or statistical probability. When visualized, the tree forms the outline of its physical namesake. Trees consist of branches and leaves (or decision nodes and terminal nodes). Each branch contains a decision point which represents a test on a feature. Each leaf node represents the decision made after computing all features. Leaves have no further decisions and are the furthest points on the tree. The path through the tree from the root node (or first node) to an individual leaf node represents a classification path. Building out this classification path will be what the project team will use to determine if an email is spam or not-spam.

# Data Acquisition/Cleaning
In order to determine if an email is spam or not-spam a corpus of emails needs to be collected and attributes on that corpus defined. From this, a model can be constructed that can be later applied to additional email data to predict if it is a spam message or not. For this report, the corpus of emails has been provided. At a high level the corpus contains 9,348 records with 30 different features. The data summary table below provides a good overview of the data set. The following summary of insights is apparent in the table:

* There are 17 categorical variables in the data set
* There are 13 continuous variables in the data set
* At least four features (subExcCt, subQuestCt, numRec, and subBlanks) have missing values that need to be addressed

With respect to the missing values, for the purposes of this analysis the rows with missing values will be dropped. This will bring our working record count to 9045.
```{r, echo=FALSE, message=FALSE}

# Load necessary package libraries
library(data.table)
library(tidyverse)
#install.packages("caret")
library(caret)
library(rpart)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("randomForest")
library(randomForest)
#install.packages("gbm")
library(gbm)
#install.packages("corrgram")
library(corrgram)
library(corrplot)
#install.packages("skimr")
library(skimr)
#install.packages("rattle")
library(rattle)
library(RColorBrewer)
#install.packages("ROCR")
library(ROCR)
#install.packages("broom")
library(broom)
#install.packages("e1071")
library("e1071")
```

```{r, echo=FALSE, message=FALSE}
# Define Helper functions

detectNA <- function(inp) {
  sum(is.na(inp))
}

detectCor <- function(x) {
  cor(as.numeric(emailDFrp[, x]), 
    as.numeric(emailDFrp$isSpam), 
    method="spearman")
}

```

```{r, echo=FALSE, message=FALSE}
# Load data
croom_wd = "C:/RAI/DS7333-QTW/CaseStudy3/"
barker_wd = "F:/SMU/DS7333/Repo/DS7333-QTW/CaseStudy3"
weinstock_wd = ""

setwd(barker_wd)

load("data.rda")
```

```{r, echo=FALSE, message=FALSE}
 # summarize the data to look at counts, datatypes, and other information.
 skim(emailDFrp)
```

```{r, echo=FALSE, message=FALSE, results='hide'}
#omit and NAs if found in the data set
emailDFrp = na.omit(emailDFrp)
skim(emailDFrp)
```
Continuing in the exploratory data analysis, the data set has a feature called isSpam. This feature is the original classification value for each email to determine whether it is spam or not. For the purposes of building out a classification model an evaluation needs to occur on this feature to see whether the data is balanced or unbalanced. Balanced data indicates that the feature values have approximately a 50/50 split in them. Unbalanced data indicates that the feature values skew in one direction or another. As Figure 1 below displays, the isSpam field is unbalanced in that there are more records in the data set classified with isSpam=F which indicates a not-spam email versus a spam email where isSpam=T. From a numeric perspective, 6674 of the 9045 records are classified as not spam, while 2371 are classified as spam.

```{r, echo=FALSE, message=FALSE}
#Look at the frequency distribution of spam in our data
dfrQltyFreq <- summarise(group_by(emailDFrp, isSpam), count=n())

# plot the number of spam vs non-spam message in the data
ggplot(dfrQltyFreq, aes(x=isSpam, y=count)) +
    geom_bar(stat="identity", aes(fill=count)) +
    labs(title="Figure 1: isSpam Feature Frequency Distribution") +
    labs(x="isSpam") +
    labs(y="Counts")
```

As we continue to explore the data, there are certain combinations of fields, along with isSpam, that only return a single value of isSpam. Three charts below, figure 1b, and figure 1c, demonstrate the singular value if isSpam under certain value conditions for that attribute. We think this might be useful in tree creation especially with decision splits. Additionally, the field, isPGPSigned when true, is always not spam - which makes sense as PGPSigned is an authentication protocol used to verify the sender of the email. This field may also play prominentely in tree creation and pruning.

```{r}
emailDFrp %>% 
  ggplot(mapping = aes(x= isInReplyTo, fill = isSpam))+
  geom_bar() + 
  labs(title="Figure 1b: isSpam vs. isinReplyTo Values") +
    labs(x="isinReplyTo") +
    labs(y="Counts")

```
```{r}
emailDFrp %>% 
  ggplot(mapping = aes(x= priority, fill = isSpam))+
  geom_bar() +
  labs(title="Figure 1c: isSpam vs. priority Values") +
  labs(x="Priority") +
  labs(y="Counts")
```

```{r}
emailDFrp %>% 
  ggplot(mapping = aes(x= isPGPsigned, fill = isSpam))+
  geom_bar() + 
  labs(title="Figure 1d: isSpam vs. isPGPSigned Values") +
  labs(x="isPGPSigned") +
  labs(y="Counts")
```


Further exploring the data, the investigation of correlations between features needs to occur. Correlations are relationships between two or more features. Understanding which features in the dataset are correlated allow for data set simplification. If fields are highly correlated then similar enough information may be present that both features do not need to be carried forward into analysis. The correlation plots below show the correlations between all 30 features in the data set.

```{r, echo=FALSE, message=FALSE, results='hide'}

# find correlations
Corr_DF <- abs(sapply(colnames(emailDFrp), detectCor)) #absolute value

summary(Corr_DF)
```
```{r, echo=FALSE, message=FALSE}

# One Hot Encode the dataframe to run correlation plots
OHE_emailDRrp = copy(emailDFrp)

cols <- sapply(OHE_emailDRrp, is.factor)

#convert all the factors to character values
OHE_emailDRrp[,cols] <- lapply(OHE_emailDRrp[,cols], function(x) as.character(x))

#convert the character value t/f to 1/0 respectively
OHE_emailDRrp[,cols] <- lapply(OHE_emailDRrp[,cols], function(x) gsub("T", "1", x))
OHE_emailDRrp[,cols] <- lapply(OHE_emailDRrp[,cols], function(x) gsub("F", "0", x))

#convert the character 1/0 to numberis
OHE_emailDRrp[,cols] <- lapply(OHE_emailDRrp[,cols], function(x) as.numeric(x))


corrplot(cor(OHE_emailDRrp[c(2:10,1)]))
corrplot(cor(OHE_emailDRrp[c(10:20,1)]))
corrplot(cor(OHE_emailDRrp[c(20:30,1)]))

```
From the correlation plot thre are some features that show high correlation. In summary, highly correlated features are

* isInReplyTo and isRE
* numLines and bodyCharCt
* numDlr and subQuestCt

It should also be noted at this point that in order to achieve accurate correlations, the categorical factors were one hot encoded. One hot encoding simply changes categorical variables to numeric values for easier analysis. Specifically relating to this data set the categorical variables are primarily the values T and F representing True and False. The T values were converted to the numeric value 1 and F values were converted to the numeric value 0. This is a typical approach for handling True/False values. 

# Data Analysis

Moving into the data analysis phase where classification of an email as spam or not-spam can begin to occur, the first step that needs to occur is building out training and test datasets. Breaking the data out into a training and test dataset allows for model building, using the training data set, and then model evaluation can occur on the test dataset. Building out two data sets in this way assists in managing model overfitting.

We selected an 80/20 split of our test and train data set as we felt it provided enough data to accurately construct our model with.

```{r, echo=FALSE, message=FALSE}
# build out training and test data. We'll use an 80/20 split to start
set.seed(100)
cntTrainRecs = createDataPartition(y=emailDFrp$isSpam,p=0.8,list=FALSE)
train_df = emailDFrp[cntTrainRecs,]
test_df = emailDFrp[-cntTrainRecs,]

dim(train_df)
dim(test_df)
```
## Basic Tree Model

Starting off the classification of email message the first step will be to begin with a basic tree classification model. This will allow for a quick understanding of classification and provide a baseline for further comparison.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
model = rpart(formula=isSpam~., data=train_df,method="class")
fancyRpartPlot(model,caption="Initial Tree")
```
As shown above the tree is relatively large but does provide an idea about how an email message may be classified as spam or not-spam. Using this base tree the overall accuracy of the model can be evaluated. Again this will help determine a baseline for analysis as future trees are developed. 

In evaluating this tree, the first node starts with the perCaps feature. If perCaps is less than 1.3 the values move to the left of the tree. If they are greater they move to the right of the tree. Following the right side of the tree, the next test is the numLines feature. If the numLines is less than 1.2 the data moves to the left node and those values greater move to the right. Continuing right from numLines, the isInReplyTo feature is tested. If this feature is true the email is classified as spam. If it is false then the email is classified as not-spam. Following a simlar process as outlined above can be done for all nodes in the tree and will provide an indication on how the new emails may be classified.

We can also clearly see that isinReplyTo is a major decision node for this tree as postulated above during our EDA processing.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
predTest = predict(object=model, newdata=test_df, type="class")
confusionMatrix(data=predTest, reference=test_df$isSpam)
```
From the confusion matrix above the overall accuracy of the first tree configuration is approximately 91%. Of the 1809 records in the test data set, 1271 were accurately classified as not-spam and 376 were accurately classified as spam. 

To help with any possible over fitting the tree can be pruned. Pruning trees attempts to get to a more optimal tree by removing nodes in the tree and stopping at what the modeler deems is an acceptable stopping criteria. To determine what that stopping criteria could be, investigation of the error in the original tree is helpful. The plot below shows the cross validation error for each split that can be used to prune the tree. From the plot the cp equal to approximately 0.014 would be a good spot to prune the tree.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
 plotcp(model)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
pruneModel = prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
fancyRpartPlot(pruneModel,caption="Pruned Tree")
```

The pruned tree looks very similar to the initial tree. This is also confirmed by the confusion matrix results shown below. No change is seen between the initial tree and the pruned tree. The overall accuracy of the pruned tree configuration is approximately 91%. Of the 1809 records in the test data set, 1271 were accurately classified as not-spam and 376 were accurately classified as spam. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
predTest = predict(object=pruneModel, newdata=test_df, type="class")
confusionMatrix(data=predTest, reference=test_df$isSpam)
```
The lack of change between the original tree and pruned tree could be due to the imbalanced classes as related to the isSpam feature. As noted previously, there are more non-spam emails than spam emails in the data set. This could be impact the overall model accuracy. In order to overcome this possible over fitting evaluating the data leveraging a cross fold validation method may be called for. The cross fold validation method will also assist in dealing with the class inequality in the spam and not-spam values. 

In cross fold validation, the dataset is broken up into a number of groups. Each of these groups is called a fold, denoted by the value 'k'. Each fold is then used as a testing and training data set. For purposes of this exercise k will be equal to 5 and this will be repeated 5 times. 

```{r,echo=FALSE,message=FALSE}
set.seed(100)
cv5 = createMultiFolds(train_df$isSpam,k=5,times=5)
control = trainControl(method="repeatedcv", number=5, repeats=5, index=cv5)

modelCV = train(x=train_df[,-1],y=train_df[,1], method="rpart", tuneLength=30,trControl=control)

fancyRpartPlot(modelCV$finalModel, caption="CV Tree - Initial")
```
Using this new tree development approach resulted in a much larger tree than the first tree build. Evaluating the confusion matrix, shown below, of this new approach will provide guidance on whether the predictions are getting better or worse.

```{r,echo=FALSE,message=FALSE}
predTest = predict(object=modelCV$finalModel, newdata=test_df, type="class")
confusionMatrix(data=predTest, reference=test_df$isSpam)
```

The confusion matrix for the cross fold validation approach resulted in a higher accuracy of approximately 93%. Of the 1809 records in the test data set, 1296 were accurately classified as not-spam and 401 were accurately classified as spam. Taking this approach has resulted in better accuracy, however there are still other methods that will provide better results with respect to other metrics that will provide more confidence in classifying emails as spam or not-spam.

## Random Forest

Another approach that can be leveraged for classification is Random Forest. Using Random Forest multiple trees are built that are compared together to evaluate which tree is the best. Additionally, with Random Forest the most important variables that impact the tree creation can be evaluated. For the analysis of Random Forest 500 trees will be generated and the default for classification of using the square root of the number of features will be used when evaluating the number of features used in the construction of each tree. The model summary is presented below.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
#Build out random forest for analysis
set.seed(100)

#Mtry = sqrt(ncol(emailDFrp)-2)
Ntrees = 500

#track the processing time for grins
startTime = proc.time()

RFModel = randomForest(isSpam~.-isSpam,data=train_df,ntree=Ntrees, importance=TRUE)

endTime = proc.time()

```

```{r,echo=FALSE,message=FALSE}
# Display the model for accuracy understanding
RFModel
```

From the model summary on the training data it can be seen that 500 trees were build with 5 variables attempted at each split. The random forest was able to classify 5281 of the non-spam emails correctly, and 1791 of the spam emails correctly. 

Continuing to dig into the random forest models, the important variables can be determined. Variable importance is presented two different ways in the plots below. The first plot represents the top 15 features in descending importance based on the mean decrease accuracy metric. The mean decrease accuracy metric describes the impact each feature has on the model if it is removed from the model. Similarly, the second plot represents the top 15 features in descending importance based on the mean decrease gini. The mean decrease gini evaluates the model based on how each feature impacts the gini index that is calculated. 

In looking across both feature importance plots a few key observations are present:

* The perCaps feature is the most important
* Both plots contain a lot of similarities in important features

```{r,echo=FALSE,message=FALSE}
# Determine which factors have the most importance
#importance(RFModel)
#NOTE: Plotting the top 15 variables here for readability. most trail off under 30ish
varImpPlot(RFModel,main="Variable Importance Plot",n.var=15)
```

Now that there is an understanding of how accurate the Random Forest model is on the training data set and what features are important to the model, the random forest model can now be run on the test data set to determine how accurate the model is. The confusion matrix below indicates the results. 

```{r,echo=FALSE,message==FALSE}
# Build out a confusion matrix for analysis
predictVal <- predict(RFModel, newdata=test_df)
CM_Result  <- confusionMatrix(predictVal,test_df$isSpam)
CM_Result
```

Utilizing random forest the model accuracy has increased to 97%. The model successfully classified 1321 non-spam emails as spam and 443 spam emails as spam. Indications seems to indicate that this model is better than the initial tree based models that were built. To a degree this is expected since a random forest classifier is building multiple trees versus the single trees in the initial outset. 


## Grid Search 

Another approach that could be used to build a tree is the grid search method to aid us with hyperparameter tuning. In the code below, we specify a range of hyperparamter settings (in this case minsplit and maxdepth) and then iterate through them to see which displays the best model. Hyperparameter tuning is useful because it helps us quickly identify the likely best parameters to use in construction of our decision tree. In future iterations to speed up analysis, other grid search methods can be used. We tried 'Ranger' which significantly sped up processing time, but did not improve the results - so it was not included in our analysis.

```{r, echo=FALSE, message=FALSE}

# Defines our parameters that will be used
gs <- list(minsplit = c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30),
           maxdepth = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)) %>% 
  cross_df() # Convert to data frame

# Our tree function
mod <- function(...) {
  rpart(formula=isSpam~., data=train_df, ,method="class", control = rpart.control(...))
}

gs <- gs %>% mutate(fit = pmap(gs, mod))


```

After our grid search is done, we define an accuracy calculation and apply it against our training data set to see how well each parameter combination performed. We can see that the optimal model chosen by grid search is a minsplit of 30 with a max depth of 8. However, we can also see that a minsplit of 2 and a max depth of 13 also performs equally well when evaluating the accuracy metric. Therefore, to avoid overfitting with a deep tree, we will select minsplit of 2 and a maxdepth of 13 for our optimal grid search tree.
```{r, echo=FALSE, message=FALSE}

# Create an accuracy function to measure results
compute_accuracy <- function(fit, test_features, test_labels) {
  predicted <- predict(fit, test_features, type = "class")
  mean(predicted == test_labels)
}

test_features <- train_df %>% select(-isSpam)
test_labels   <- train_df$isSpam

gs <- gs %>%
  mutate(test_accuracy = map_dbl(fit, compute_accuracy,
                                 test_features, test_labels))

# Arrange the results to display the highest accuracy
gs <- gs %>% arrange(desc(test_accuracy), desc(minsplit), maxdepth)
gs

```


As you can see from the below, this tree doesn't look all that different from our initial trees. The marked difference however is the approach we used to define the optimal parameters using grid search to define the model. Given the accuracy score above (0.92), and the visualized model below, we can conclude that while the grid search method performs better than our initial model (0.91), the random forest model with an accuracy of 0.97 performs much better than this model and the initial model. 

```{r,echo=FALSE,message=FALSE}
gs_model = rpart(formula=isSpam~., data=train_df,method="class", minsplit = 2, maxdepth=13)
fancyRpartPlot(gs_model,caption="Grid Search Tree")
```


# Conclusion

In conclusion this report has leveraged multiple tree-based classification models to determine if an email is spam or not-spam. Initially we started with a basic classification tree and evaluated the dataset to see how it would perform. That initial tree was then pruned in an attempt to obtain a better result. In this particular instance there was no difference seen between the pruned and non-pruned trees. We then attempted to leverage random forest as a way to classify the emails. Random forest allowed for building multiple trees at once to obtain a result. The final approach we leveraged was grid search. Through grid search we attempted to use hyperparameter tuning to classify the email results and optimized our parameter selection.

Our results indicate that the random forest model performed better than all other tree models attempted; with an accuracy score of 0.97, the random forest model is a much better choice for identifying spam records in this data set compared to the other tree models attempted. One potential improvement on the random forest model would be to use grid searching to identify better performing parameters to see if the accuracy score would increase. A more balanced data set would also be useful to validate whether our models continue to be accurate.







