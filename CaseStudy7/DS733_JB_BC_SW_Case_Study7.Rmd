---
title: 'MSDS 7333 Fall 2020: Final Case Study'
author: "Jayson Barker, Brandon Croom, Shane Winestock"
output:
  pdf_document: default
  html_document: default
---

# Business Understanding

The goal of this case is to provide business clients an understanding of how to approach a data science problem and to minimize dollar cost based on those data science decisions. For this case the following information has been provided:

* A data set with unknown data
* A request to build a model that balances minimizing cost to the business but is accurate enough to apply to the business problem

In addition to the data, information on the cost of decisions has also been provided. The decision cost breaks down as follows:

* True Positives - $0
* True Negatives - $0
* False Positive - $10
* False Negative - $500

This information allows for understanding that correctly classifying the data does not cost the company anything. Classifying the data as true when it is false (False Positive) costs the company $10. Classifying the data as false when it is true (False Negative) costs the company $500. As the classification models are built one of the key goals will be to minimize the cost associated with these values. 

For the purposes of this analysis two metrics will be leveraged to compare the classification models: accuracy and classification cost. The accuracy metric will identify how well the model performed in classifying the data values. High accuracy means either the model is performing well of the model is over-fitting on the data. Data over-fitting is detrimental for analysis purposes in that new data may not get classified correctly. This is where the classification cost metric comes into the picture. Using the information provided above, a classification cost for each model will be calculated. This will allow for an understanding of the impact poor classification has from a financial perspective. The combination of these two metrics should provide good initial guidance for model selection. 

# Initial Data Understanding

The first step to approaching a data science problem is to understand the data that is available for use. In this particular case a data set of unknown origin / contents has been provided. For this data set, there is no metadata nor column headers available to make data inferences from. After loading the data, the data types will be evaluated. This will provide an understanding of how the data columns are structured and begin to paint a picture for analysis. 

```{r libLoad, message=FALSE,warning=FALSE,results='hide',echo=FALSE}
# Load necessary libraries for analysis
library(data.table)
library(tidyverse)
library(ggplot2)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)
library(class)
#install.packages("mice")
library(mice)
#install.packages("ranger")
library(ranger)

#install.packages("gmodels")
library(gmodels)

#Code to install Data Explorer below
#f (!require(devtools)) install.packages("devtools")
#devtools::install_github("boxuancui/DataExplorer")
library(DataExplorer)
```

```{r dataLoad, message=FALSE, warning=FALSE, echo=FALSE}
# read in the data file
df = read.csv(file.choose(),header=TRUE)
```

In evaluating the data, the following summary information can be obtained. There are:

```{r, message=FALSE, warning=FALSE, echo=FALSE}
output = introduce(df)
output = cbind(variable=names(output),t(output))
as.data.table(output)
```
The output below shows these columns are the discrete columns (noted as character) and which are continuous (noted as numeric).

```{r, message=FALSE, warning=FALSE, echo=FALSE}
output <- data.frame(sapply(df,class))                    
output <- cbind(variable = names(df), output)
as.data.table(output)
```
In evaluating the missing data, the following 5 plots will assist in understanding the percentages of missing data by individual column. The plots indicate there is at most 0.03% of the data missing in any column. This would indicate that there are not large swaths of data missing in columns.   

```{r, echo=FALSE,message=FALSE, warning=FALSE}
plot_missing(df[1:10], title = "Plot of Missing Values: 1 - 10")
plot_missing(df[11:20], title = "Plot of Missing Values: 11 - 20")
plot_missing(df[21:30],title = "Plot of Missing Values: 21 - 30")
plot_missing(df[31:40],title = "Plot of Missing Values: 31 - 40")
plot_missing(df[41:51],title = "Plot of Missing Values: 41 - 51")
```

Now that the amount of missing data in the data set is understood, the evaluation of the discrete and continuous variables must be undertaken. The discrete variables will be addressed first. From the bar charts below the buckets that each data value is associated with is shown. This allows for additional data understanding. For example, in evaluating variable x29, events seem to occur more in the summer months (June, July, August). Similarly for variable x30, events seem to occur more on Wednesdays than other days of the week. This analysis also points out a possible issue with data cleaning that needs to occur. Variable x32 looks to actually be a numeric value, but is being treated as categorical due to the percentage size inside the data. That value will be stripped out to make it more appropriate for analysis. Additionally, this analysis provides a quick look into the categorization variable (y). From the bar chart, it can be seen that there is not a large class imbalance in the variable. If such an imbalance existed additional steps would need to be undertaken to address this prior to analysis. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
 plot_bar(df,title="Bar Plots of Discrete Variables")
```
Moving on to the continuous variables, histogram plots will be leveraged for analysis. Histogram plots will provide an understanding of the data distribution. From the histogram plots below, the data look to be normally distributed for all of the continuous variables. This indicates there are no data outliers that need to be addressed and no data wrangling that needs to occur for correction.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
 plot_histogram(df, title="Histogram Plots of Continuous Variables")
```
Evaluating all of the plots for both the discrete and continuous variables also shows that one value, x37, is missing from any of the plots. Upon investigation that field seems to be a currency field. This field contains the dollar sign ($) symbol which is not getting picked up correctly. 

To summarize the data discovery portion of this case:
* There are a few elements of missing data that will need to be cleaned up
* There are characters in the data that need to be cleaned up.

## Addressing Data Issues

As discussed previously there are characters in values that should be treated as numeric. Specifically in variable x32 there is a percentage sign and in variable x37 there is a dollar sign. If these characters are not removed from the analysis then there will be issues with model building.

```{r, echo=FALSE, message=FALSE, warning=FALSE, output=FALSE}
df <- df %>% mutate(x32 = as.numeric(gsub("%", "", x32)))
df <- df %>% mutate(x37 = as.numeric(gsub("\\$", "", x37)))

df %>% 
  mutate(across(where(is.character), str_trim))

```
The key data issue that needs to be addressed is the missing data. As previously noted there are 1466 missing values across the data set. It seems that the data is missing at random. This missing data approach provides flexibility in that missing data can be excluded from the observations without dramatic impact. This missing data represents only 1% of the overall data set. However, to ensure as much data as possible is maintained an imputation attempt will be executed on the missing data. The plots below show the results of the imputation and show that there is less missing data in the data set. A couple of variables, x2 and x41, still maintain some missing data. Given the 1% impact, the final step will be to remove the missing data. Further investigation may be needed once a production model is chosen to build out the imputation of these specific variables. 

```{r, echo=FALSE,message=FALSE, warning=FALSE,results='hide',output=FALSE}
# Below we use the MICE package to impute the missing values
# Due to the size of the data, the number of iterations will be limited 
imp = mice(df, method = "pmm", print = FALSE, minbucket = 2)
#plot(imp)
#imputed.df = mice::complete(imp, 1)
#imp$method

#plot_missing(imputed.df[1:10], title = "Plot of Missing Values: 1 - 10")
#plot_missing(imputed.df[11:20], title = "Plot of Missing Values: 11 - 20")
#plot_missing(imputed.df[21:30],title = "Plot of Missing Values: 21 - 30")
#plot_missing(imputed.df[31:40],title = "Plot of Missing Values: 31 - 40")
#plot_missing(imputed.df[41:51],title = "Plot of Missing Values: 41 - 51")

# pass to df
#df = imputed.df
```
```{r,echo=FALSE,message=FALSE, warning=FALSE}
plot(imp)
imputed.df = mice::complete(imp, 1)
imp$method

plot_missing(imputed.df[1:10], title = "Plot of Missing Values: 1 - 10")
plot_missing(imputed.df[11:20], title = "Plot of Missing Values: 11 - 20")
plot_missing(imputed.df[21:30],title = "Plot of Missing Values: 21 - 30")
plot_missing(imputed.df[31:40],title = "Plot of Missing Values: 31 - 40")
plot_missing(imputed.df[41:51],title = "Plot of Missing Values: 41 - 51")

# pass to df
df = imputed.df
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
 df = na.omit(df)
```


## Data Cleanup Verification

Now that all data has been cleaned and confirmed to be appropriate, the analysis for discrete and continuous variables will be executed to verify that everything is cleaned as it should be. First, the bar plots of the discrete variables will be executed. Finally, the histogram plots for the continuous variables will be executed. 

First, re-evaluate the data set at a high level to ensure that the original variables that needed cleaning show as expected.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
output = introduce(df)
output = cbind(variable=names(output),t(output))
as.data.table(output)
```

Evaluating the discrete value plots indicates that variable x32 has been categorized as continuous instead of discrete. This gives us 3 discrete variables for evaluation. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
 plot_bar(df,title="Bar Plots of Discrete Variables")
```
Reviewing the continuous variables, variable x37 is now showing as a continuous variable which indicates that data correction has been completed. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
 plot_histogram(df,title="Bar Plots of Continuous Variables")
```
The final portion of data understanding to address is to evaluate the correlation between variables. Evaluating correlation allows for understanding if variables are related to one another. This correlation between variables may allow for the removal of highly correlated variables. The same information will be understood if only one of the variables is present thus the model may be simplified by removing a variable.

Given the number of variables in the data a full correlation plot will be difficult to read. The correlations plots will be broken down into the discrete and continuous variables. The plot below shows the correlation plot for the discrete variables. From the plot a few items are seen:
* There is a negative correlation between the x24 values of Europe and Asia
* There is a negative correlation between x30 values for Wednesday, Monday and Tuesday
* The remainder of the values have very little correlation

```{r, echo=FALSE,message=FALSE, warning=FALSE}
plot_correlation(df,type="discrete", title="Correlation Plot: Discrete Variables")
```

The plot below shows the correlation plot for the continuous variables. From the plot below there is high correlation between variables x2 and x6, as well as between x38 and x41. 

```{r, echo=FALSE,message=FALSE, warning=FALSE}
plot_correlation(df,type="continuous", title="Correlation Plot: Continuous Variables")
```

# Modeling Approach

As part of the modeling approach a training data set and testing data set will need to be created. Creating these individual data sets will allow testing the models against a set of data, the training set, and validating that model with the testing set. This approach will allow for validation that models are performing appropriately and allow for model comparison. In building out the training and test data sets an 80/20 split will be leveraged. This will put 80% of the data in the training data set and 20% in the test data set. With this approach the training set and test set will have the values specified below. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(100)
cntTrainRecs = createDataPartition(y=df$y,p=0.8,list=FALSE)
train_df = df[cntTrainRecs,]
test_df = df[-cntTrainRecs,]


print(paste("Training Set Size: " ,dim(train_df)[1]))
print(paste("Test Set Size: " ,dim(test_df)[1]))
```

Given that the problem provided is a binary classification problem there are multiple approaches that can be taken to model the problem. For this case K-Nearest Neighbor, Logistic Regression and Random Forest models will be executed. Each of these approaches are relatively quick to execute and the outputs are easily explainable.

## KNN Modeling

The K-Nearest Neighbor (KNN) modeling method method is an approached used for both classification and regression problems. In this particular case, KNN will be leveraged for the classification problem. In the classification execution, the model attempts to group values based on a weight provided identified as K. The K value tells the model how many neighbors to attempt to model against the dependent variable. For this implementation K = 15.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Removing the character column values
train_df_knn <- train_df[, -c(25,30,31)]
test_df_knn <- test_df[, -c(25,30,31)]

# Execute KNN
prc_test_pred <- knn(train = train_df_knn, test = test_df_knn, cl = train_df$y, k=15)
```

The results of our KNN model is shown below. We compare the results from our predicted KNN model against the actual test values for our dependent variable. From it, we can see across a total of 31,984 observations (test set), we have a true positive of  of: 17,095 and a true negative of: 9,740 = for a total accuracy of 83.901%. Using KNN is limited for this data set as there were character columns omitted from the model due to limitations. Had those been factored in, we'd likely see a score increase. KNN is likely not the optimal modeling choice here, but is a good first pass to identify that our character columns may have some role to play in improving the accuracy of the model.

The true cost of implementing this model, if selected, would be $22,800 + $1,434,500 = $1,457,300. This further substantiates that KNN is likely not the model of choice if this rate / cost due to false negatives and positives is prohibitive.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
CrossTable(x=test_df$y, y = prc_test_pred, prop.chisq = FALSE)
```

## Logistic Regression

Logistic Regression is another approach to handling a classification problem. Logistic Regression is a statistical model that uses a logistic function to model a binary dependent variable. In comparison to KNN, Logistic Regression is computationally much faster and provides quick insight into a problem. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
logRegress = glm(formula= y~., family=binomial, data=train_df)
```

The plot below shows the variable importance plot. The purpose of this plot is to show which variables have the most impact on the logistic regression analysis. In this case, the top 15 features are shown. As can be seen from the plot, the x29 variable broken out by each month have the highest variable importance. 

```{r,echo=FALSE,message=FALSE,warning=FALSE}
# get variable importance
lr_vi = varImp(logRegress) %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  arrange(Overall) %>%
  mutate(rowname = forcats::fct_inorder(rowname))


lr_vi = head(lr_vi,15)

ggplot(lr_vi) +
ggtitle("Variable Importance: Logistic Regression") + 
   geom_col(aes(x = rowname, y = Overall))+
   coord_flip()+
   theme_bw()
```

Utilizing the confusion matrix below the accuracy of this model, across 31,984 observations is 70.20% (15,943 + 6,510)/31,984. Applying the Classification Cost penalty specified in the business requirements document, we have a total cost of implementation of: $34,320 + $3,049,500 = $3,083,820. This is higher than the KNN model demonstrated above and if this proves cost-prohibitive, another model may be the better choice.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Predict
predictLogRegress = predict(logRegress,test_df,type="response")

#output the confusion matrix
table(test_df$y,predictLogRegress >= 0.5)
```

## Basic Tree Model

A Basic Tree Model is another approach that can be leveraged for classification problems. In a basic tree approach a series of "if-then" rules to generate a prediction. Tree Based Models are relatively fast to develop, similar to logistic regression. These types of models are also easily explainable in that the tree structure can be followed to understand the decision making process. Trees can be prone to over fitting if not managed correctly and thus need to be pruned to ensure a best fit model. 

The plot below shows the first tree model that has been build off of the data provided. From the model it can be seen that x23 is the first decision point. From there x43 or x28 is evaluated. This process continues until a result has been obtained.  
```{r, echo=FALSE, message=FALSE, warning=FALSE}
model = rpart(formula=y~., data=train_df,method="class")
fancyRpartPlot(model,caption="Initial Tree")
```

The table below shows that the initial tree has an accuracy of 74.21%, lower than the KNN model but higher than the logistic regression model. Applying the Classification Cost penalty specified in the business requirements document, we have a total cost of implementation of: $47,140 + $1,815,140 = $1,862,280. This is higher than the Logistic Regression model demonstrated above and if this proves cost-prohibitive, another model may be the better choice.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
predTest = predict(object=model, newdata=test_df, type="class")
confusionMatrix(data=predTest, reference=as.factor(test_df$y))
```

To help with any possible over fitting, the tree can be pruned. The method of pruning trees is a process that attempts to get our results to a more optimal tree by removing nodes in the tree and stopping at what the modeler deems is an acceptable stopping criteria. To determine what that stopping criteria could be, investigation of the error in the original tree is helpful. The plot below shows the cross validation error for each split that can be used to prune the tree. From the plot the cp equal to approximately 0.014 would be a good spot to prune the tree.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
 plotcp(model)
```

The pruned tree, below, looks very similar to the initial tree. This result is also confirmed by the confusion matrix results shown below. No change is seen between the initial tree and the pruned tree. This would result in the same cost calculation as the initial tree of $1,862,280.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
pruneModel = prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
fancyRpartPlot(pruneModel,caption="Pruned Tree")
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}
predTest = predict(object=pruneModel, newdata=test_df, type="class")
confusionMatrix(data=predTest, reference=as.factor(test_df$y))
```

## Random Forest

The final classification approach for this data will be a Random Forest. Random Forests are an ensemble learning method that can be used for classification, regression and other tasks. Random Forests work by building multiple decision trees at a time and then outputting the tree that is the mode (middle) of all of the calculated trees. Given the multiple trees generated Random Forest models are a little more difficult to explain, however many of the same metrics used the the previous classification methods can be obtained. As before the Random Forest will be build off of the training data set and the information will be used to predict values in the test data set to determine the model accuracy. For the first run, the Random Forest will execute on 500 trees. This is the default value for the the model and will provide a good starting point for analysis

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide',output=FALSE}
#NOTE - This takes about 9 minutes to run
rg <- ranger(y ~ ., data = train_df,classification=TRUE,importance='impurity')
pred <- predict(rg, data = test_df)
```

Just as with the Logistic Regression model, the Random Forest model will allow for evaluating which features had the greatest impact on the model development. In comparison of the two plots for Linear Regression and Random Forest, features x29 and x43 being having consistent impact across both models. This would indicate that upon further refinement of models these would be key features to ensure remained in any future models. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
rg_vi = rg$variable.importance
rg_vi = sort(rg_vi)
barplot(head(rg_vi,15), horiz = TRUE, las = 1,main= "Variable Importance Plot: Random Forest")
```

The table below shows that the Random Forest model with 500 trees has an accuracy of 92.63% (1 - the OOB Prediction error) , the highest of all models so far. Applying the Classification Cost penalty specified in the business requirements document, we have a total cost of implementation of: $8,230 + $752,500 = $760,730. This is lower than other models to this point and seems to indicate this may be a good first model for evaluation.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
rg
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
table(test_df$y,pred$predictions)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE,results='hide',output=FALSE}
#NOTE - this takes about 30 minutes to run
rg <- ranger(y ~ ., data = train_df,classification=TRUE,importance='impurity',num.trees = 1000)
pred <- predict(rg, data = test_df)
```

The table below shows that the Random Forest model with 1,000 trees has an accuracy of 92.69% (1 - the OOB Prediction error), the highest of all models. Applying the Classification Cost penalty specified in the business requirements document, we have a total cost of implementation of: $8,090 + $753,500 = $761,590. This less than a $1,000 difference from the Random Forest of 500 trees. Due to the number of trees, this model does take much longer (approx. 20 - 30 minutes) to run compared to the 500 tree model (approx. 10 minutes). If speed of processing is a business requirement sacrificing 0.06% accuracy at the same Classification Cost penalty may be acceptable. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
rg
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
table(test_df$y,pred$predictions)
```

# Conclusion

In conclusion, this report has leveraged multiple classification methods to classify an unknown data set. Initially, the data was investigated to evaluate and address missing data and overall data cleanliness. Once the data was ready for processing, four difference classification models were leveraged for comparison on performance. The models were compared on two metrics: overall model accuracy and model classification cost. A summary of the models and the associated metrics is shown below:

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl = "
| Method                     | Accuracy   | Classification Cost |
|---------------|:----------:|-----------:|--------------------:|
| K-Nearest Neighbor         |   83.90%   |    $1,457,300       |
| Logistic Regression        |   70.20%   |    $3,083,820       |
| Basic Tree                 |   74.21%   |    $1,862,280       |
| Random Forest (500 trees)  |   92.63%   |    $760,730         |
| Random Forest (1000 trees) |   92.69%   |    $761,590         |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

From these current modeling initiatives the Random Forest model seems to be the best model with the lowest Classification Cost penalty - at $761,590. This is substantially lower cost than the KNN, Logistic Regression, and Basic Tree models performed. If the business is looking to keep costs low and maintain a high-level of accuracy in the model selected, we recommend using Random Forest with 1,000 trees. If explainability of the model is of concern however, this choice provides some good indicators but may be difficult to provide full transparency of the model if that is required. Thus another model with more transparency may be used, but could have a higher Classification Cost.